# Training Config File

# Sets which agent algorithm framework will be used.
# Options are:
# "SB3" (Stable Baselines3)
# "RLLIB" (Ray RLlib)
# "CUSTOM" (Custom Agent)
agent_framework: RLLIB

# Sets which deep learning framework will be used (by RLlib ONLY).
# Default is TF (Tensorflow).
# Options are:
# "TF" (Tensorflow)
# "TF2" (Tensorflow 2.X)
# "TORCH" (PyTorch)
deep_learning_framework: TF2

# Sets which Agent class will be used.
# Options are:
# "A2C" (Advantage Actor Critic coupled with either SB3 or RLLIB agent_framework)
# "PPO" (Proximal Policy Optimization coupled with either SB3 or RLLIB agent_framework)
# "HARDCODED" (The HardCoded agents coupled with an ACL or NODE action_type)
# "DO_NOTHING" (The DoNothing agents coupled with an ACL or NODE action_type)
# "RANDOM" (primaite.agents.simple.RandomAgent)
# "DUMMY" (primaite.agents.simple.DummyAgent)
agent_identifier: PPO

# Sets whether Red Agent POL and IER is randomised.
# Options are:
# True
# False
random_red_agent: False

# Sets what view of the environment the deterministic hardcoded agent has. The default is BASIC.
# Options are:
# "BASIC" (The current observation space only)
# "FULL" (Full environment view with actions taken and reward feedback)
hard_coded_agent_view: FULL

# Sets how the Action Space is defined:
# "NODE"
# "ACL"
# "ANY" (node and ACL actions)
action_type: NODE

# Observation space
observation_space:
  components:
    - name: NODE_LINK_TABLE

# Number of episodes for training to run per session
num_train_episodes: 1000

# Number of time steps for training per episode
num_train_steps: 200

# Number of episodes for evaluation to run per session
num_eval_episodes: 1

# Number of time steps for evaluation per episode
num_eval_steps: 17

# Sets how often the agent will save a checkpoint (every n time episodes).
# Set to 0 if no checkpoints are required. Default is 10
checkpoint_every_n_episodes: 10

# Time delay (milliseconds) between steps for CUSTOM agents.
time_delay: 5

# Type of session to be run. Options are:
# "TRAIN" (Trains an agent)
# "EVAL" (Evaluates an agent)
# "TRAIN_EVAL" (Trains then evaluates an agent)
session_type: TRAIN_EVAL

# Environment config values
# The high value for the observation space
observation_space_high_value: 1000000000

# The Stable Baselines3 learn/eval output verbosity level:
# Options are:
# "NONE" (No Output)
# "INFO" (Info Messages (such as devices and wrappers used))
# "DEBUG" (All Messages)
sb3_output_verbose_level: INFO

# Reward values
# Generic
all_ok: 0                        # Neutral reward when everything is fine
# Node Hardware State
off_should_be_on: -20             # Strong penalty for being off when it should be on
off_should_be_resetting: -10      # Moderate penalty for being off when it should be resetting
on_should_be_off: -5              # Small penalty for being on when it should be off
on_should_be_resetting: -10       # Moderate penalty for being on when it should be resetting
resetting_should_be_on: -10       # Moderate penalty for resetting when it should be on
resetting_should_be_off: -5       # Small penalty for resetting when it should be off
resetting: -3                     # Minor penalty for being in the resetting state
# Node Software or Service State
good_should_be_patching: 10       # Higher reward for transitioning to the patching state when needed
good_should_be_compromised: -40   # Strong penalty for transitioning from a good state to compromised
good_should_be_overwhelmed: -40   # Strong penalty for transitioning from a good state to overwhelmed
patching_should_be_good: 20       # Higher reward for successfully patching a service back to good
patching_should_be_compromised: -20 # Moderate penalty for patching but still becoming compromised
patching_should_be_overwhelmed: -20 # Moderate penalty for patching but still becoming overwhelmed
patching: 10                      # Reward for being in the patching state
compromised_should_be_good: -50   # Very high penalty for being compromised instead of good
compromised_should_be_patching: -30 # High penalty for being compromised when patching is possible
compromised_should_be_overwhelmed: -50 # Very high penalty for letting compromise lead to overwhelmed
compromised: -50                  # Very severe penalty for remaining in a compromised state
overwhelmed_should_be_good: -50   # Very high penalty for remaining overwhelmed instead of being good
overwhelmed_should_be_patching: -30 # High penalty for overwhelmed state when patching is possible
overwhelmed_should_be_compromised: -30 # Penalty for transitioning from overwhelmed to compromised
overwhelmed: -50                  # Severe penalty for remaining in an overwhelmed state
# Node File System State
good_should_be_repairing: 15      # Increased reward for transitioning from good to repairing when needed
good_should_be_restoring: 15      # Increased reward for restoring a good state
good_should_be_corrupt: -40       # Strong penalty for allowing corruption from a good state
good_should_be_destroyed: -60     # Very high penalty for system destruction from a good state
repairing_should_be_good: 25      # High reward for successfully repairing to a good state
repairing_should_be_restoring: 10 # Moderate reward for restoring during repair
repairing_should_be_corrupt: -30  # Moderate penalty for corruption during repair
repairing_should_be_destroyed: -40 # High penalty for being destroyed during repair
repairing: 10                     # Moderate reward for repair actions
restoring_should_be_good: 20      # Higher reward for restoring to a good state
restoring_should_be_repairing: 10 # Moderate reward for repairing during restoring
restoring_should_be_corrupt: -20  # Penalty for corruption during restoring
restoring_should_be_destroyed: -30 # Penalty for destruction during restoring
restoring: -6                     # Minor penalty for being stuck in the restoring state
corrupt_should_be_good: -50       # Strong penalty for corruption instead of being good
corrupt_should_be_repairing: -30  # Penalty for corruption instead of repairing
corrupt_should_be_restoring: -30  # Penalty for corruption instead of restoring
corrupt_should_be_destroyed: -60  # Severe penalty for system destruction
corrupt: -50                      # High penalty for remaining in a corrupt state
destroyed_should_be_good: -60     # Severe penalty for system being destroyed
destroyed_should_be_repairing: -40 # High penalty for not repairing after destruction
destroyed_should_be_restoring: -40 # High penalty for not restoring after destruction
destroyed_should_be_corrupt: -60  # Severe penalty for remaining destroyed and corrupt
destroyed: -60                    # Strong penalty for complete system destruction
scanning: -5                      # Slight penalty for unnecessary scanning
# IER status
red_ier_running: -15              # Strong penalty for red agent running undetected IER
green_ier_blocked: 30             # High reward for blocking green IER attempts

# Patching / Reset durations
os_patching_duration: 5            # The time taken to patch the OS
node_reset_duration: 5             # The time taken to reset a node (hardware)
service_patching_duration: 5       # The time taken to patch a service
file_system_repairing_limit: 5     # The time taken to repair the file system
file_system_restoring_limit: 5     # The time taken to restore the file system
file_system_scanning_limit: 5      # The time taken to scan the file system
